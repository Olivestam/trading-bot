{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook loads historical stock data from the Alpaca API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load every minute stock data from todays NASDAQ 100 index companies, from 2000-01-01 to 2025-01-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca.data import StockHistoricalDataClient\n",
    "from alpaca.data.requests import StockBarsRequest\n",
    "from alpaca.data.timeframe import TimeFrame\n",
    "from pytickersymbols import PyTickerSymbols\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Keys\n",
    "API_KEY = os.getenv(\"ALPACA_API_KEY\")\n",
    "SECRET_KEY = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "\n",
    "# Create client\n",
    "stock_client = StockHistoricalDataClient(API_KEY, SECRET_KEY)\n",
    "\n",
    "# Get all stock symbols in NASDAQ 100\n",
    "stock_data = PyTickerSymbols()\n",
    "nasdaq_100 = stock_data.get_stocks_by_index('NASDAQ 100')\n",
    "\n",
    "# Create empty DataFrame\n",
    "columns = [\"symbol\", \"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trade_count\", \"vwap\"]\n",
    "stock_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "stock_nr = 0\n",
    "for stock in nasdaq_100:\n",
    "    symbol = stock[\"symbol\"]\n",
    "    stock_nr += 1\n",
    "    print(f\"Processing stock {stock_nr}/100 - {symbol}\")\n",
    "\n",
    "    # Get the stock bars\n",
    "    request_params = StockBarsRequest(symbol_or_symbols=[symbol], timeframe=TimeFrame.Minute, start=datetime(2000, 1, 1), end=datetime(2025, 1, 1))\n",
    "    bars = stock_client.get_stock_bars(request_params)\n",
    "    \n",
    "    # Check if the symbol is in the data\n",
    "    if (symbol in bars.data):\n",
    "        # Clean stock data and add to DataFrame\n",
    "        df = pd.DataFrame(bars[stock[\"symbol\"]])\n",
    "        df = df.map(lambda x: x[1]) \n",
    "        df.columns = [\"symbol\", \"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trade_count\", \"vwap\"]\n",
    "        if not df.empty:\n",
    "            stock_df = pd.concat([stock_df, df], ignore_index=True)\n",
    "\n",
    "# Save data to a parquet file\n",
    "stock_df.to_parquet(\"./data/raw/raw__nasdaq100_2000_to_2025.parquet\", engine=\"pyarrow\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load every minute data for S&P 500\n",
    "We use SPY since Alpaca do not provide S&P 500 data through thier API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca.data import StockHistoricalDataClient\n",
    "from alpaca.data.requests import StockBarsRequest\n",
    "from alpaca.data.timeframe import TimeFrame\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Keys\n",
    "API_KEY = os.getenv(\"ALPACA_API_KEY\")\n",
    "SECRET_KEY = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "\n",
    "stock_client = StockHistoricalDataClient(API_KEY,  SECRET_KEY)\n",
    "\n",
    "\n",
    "stock = \"SPY\"\n",
    "request_params = StockBarsRequest(symbol_or_symbols=[stock], timeframe=TimeFrame.Minute, start=datetime(2000, 1, 1), end=datetime(2025, 1, 1))\n",
    "bars = stock_client.get_stock_bars(request_params)\n",
    "\n",
    "# Flatten the dictionary into a DataFrame\n",
    "df = pd.DataFrame(bars[stock])\n",
    "\n",
    "# Extrahera endast värdena från tuple-paren\n",
    "df = df.map(lambda x: x[1])  # Tar bara det andra elementet i tuple:n\n",
    "\n",
    "# Sätt kolumnnamn\n",
    "df.columns = [\"symbol\", \"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trade_count\", \"vwap\"]\n",
    "\n",
    "# Save data to a parquet file\n",
    "df.to_parquet(\"./data/raw/raw__sp500_2000_to_2025.parquet\", engine=\"pyarrow\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw to Enriched\n",
    "Add features to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.volatility import AverageTrueRange\n",
    "\n",
    "def feature_engineering(df, df_sp):\n",
    "    # Convert timestamp to Unix time (int64) for efficiency\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']).astype('int64') // 10**9\n",
    "    df_sp['timestamp'] = pd.to_datetime(df_sp['timestamp']).astype('int64') // 10**9\n",
    "    \n",
    "    # Sort data by symbol and timestamp\n",
    "    df = df.sort_values(by=['symbol', 'timestamp'])\n",
    "    df_sp = df_sp.sort_values(by=['symbol', 'timestamp'])\n",
    "    \n",
    "    # Time-based features\n",
    "    df['hour'] = (df['timestamp'] // 3600) % 24  # Extract hour from timestamp\n",
    "    df['minute'] = (df['timestamp'] // 60) % 60  # Extract minute from timestamp\n",
    "    df['day_of_week'] = (df['timestamp'] // 86400) % 7  # Day of the week\n",
    "    df['is_market_open'] = ((df['hour'] == 9) & (df['minute'] >= 30)) | ((df['hour'] == 10) & (df['minute'] == 0))\n",
    "    df['is_market_close'] = ((df['hour'] == 15) & (df['minute'] >= 30)) | ((df['hour'] == 16) & (df['minute'] == 0))\n",
    "    \n",
    "    # Price change features\n",
    "    df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    df['volatility'] = df['log_return'].rolling(window=10).std()\n",
    "    \n",
    "    # Calculate RSI for each symbol (group by 'symbol')\n",
    "    rsi = df.groupby('symbol')['close'].apply(lambda x: RSIIndicator(x, window=14).rsi()).reset_index(level=0, drop=True)\n",
    "    df['rsi'] = rsi\n",
    "\n",
    "    df['roc'] = df.groupby('symbol')['close'].pct_change(periods=10)\n",
    "    atr = df.groupby('symbol').apply(lambda x: AverageTrueRange(x['high'], x['low'], x['close'], window=14).average_true_range()).reset_index(level=0, drop=True)\n",
    "    if isinstance(atr, pd.Series):\n",
    "        df['atr'] = atr\n",
    "    else:\n",
    "        df['atr'] = atr.stack().reset_index(drop=True)\n",
    "    df['hist_volatility'] = df['log_return'].rolling(20).std()\n",
    "\n",
    "    # Target variable: price movement in 10 minutes\n",
    "    df['future_close'] = df.groupby('symbol')['close'].shift(-10)\n",
    "    threshold = 0.005  # 0.5% change\n",
    "    df['target'] = np.where(df['future_close'] > (df['close'] * (1 + threshold)), 1, np.where(df['future_close'] < (df['close'] * (1 - threshold)), 2, 0))\n",
    "    df['target'] = df['target'].astype(np.int8)\n",
    "\n",
    "    # Add S&P 500 features to the main DataFrame\n",
    "    df_sp[\"sp_return_10m\"] = df_sp[\"close\"].pct_change(10)\n",
    "    df_sp[\"sp_sma\"] = df_sp[\"close\"].rolling(window=10).mean()\n",
    "    df_sp['sp_log_return'] = np.log(df_sp['close'] / df_sp['close'].shift(1))\n",
    "    df_sp['sp_volatility'] = df_sp['sp_log_return'].rolling(window=10).std()\n",
    "    df_sp = df_sp[['timestamp', 'sp_return_10m', 'sp_sma', 'sp_volatility', 'sp_log_return']]\n",
    "    df = df.merge(df_sp, on='timestamp', how='left')\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df.astype({  # Convert to optimized dtypes\n",
    "        'hour': np.int8,\n",
    "        'minute': np.int8,\n",
    "        'day_of_week': np.int8,\n",
    "        'is_market_open': np.int8,\n",
    "        'is_market_close': np.int8,\n",
    "        'target': np.int8\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aolivest\\AppData\\Local\\Temp\\ipykernel_34076\\2277599990.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['atr'] = df.groupby('symbol').apply(lambda x: AverageTrueRange(x['high'], x['low'], x['close'], window=14).average_true_range()).reset_index(level=0, drop=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the parquet file\n",
    "file_path = \"./data/raw/raw__nasdaq100_2000_to_2025.parquet\"\n",
    "nasdaq100_data = pd.read_parquet(file_path, engine=\"pyarrow\")\n",
    "file_path_sp = \"./data/raw/raw__sp500_2000_to_2025.parquet\"\n",
    "sp500_data = pd.read_parquet(file_path_sp, engine=\"pyarrow\")\n",
    "\n",
    "# Perform feature engineering\n",
    "df = feature_engineering(nasdaq100_data, sp500_data)\n",
    "\n",
    "# Save data to a parquet file\n",
    "df.to_parquet(\"./data/enriched/enriched__nasdaq100_2000_to_2025.parquet\", engine=\"pyarrow\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aolivest\\AppData\\Local\\Temp\\ipykernel_34076\\3556077478.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['timestamp'] = pd.to_datetime(df_filtered['timestamp'])\n",
      "C:\\Users\\aolivest\\AppData\\Local\\Temp\\ipykernel_34076\\3556077478.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['time'] = df_filtered['timestamp'].dt.time\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Read the parquet file\n",
    "file_path = \"./data/raw/raw__nasdaq100_2000_to_2025.parquet\"\n",
    "df = pd.read_parquet(file_path, engine=\"pyarrow\")\n",
    "file_path_sp = \"./data/raw/raw__sp500_2000_to_2025.parquet\"\n",
    "df_sp = pd.read_parquet(file_path_sp, engine=\"pyarrow\")\n",
    "\n",
    "df_filtered = df[df[\"timestamp\"] > \"2023-12-31\"]\n",
    "df_sp = df_sp[df_sp[\"timestamp\"] > \"2023-12-31\"]\n",
    "\n",
    "# Ensure 'timestamp' is in datetime format\n",
    "df_filtered['timestamp'] = pd.to_datetime(df_filtered['timestamp'])\n",
    "df_sp['timestamp'] = pd.to_datetime(df_sp['timestamp'])\n",
    "\n",
    "# Extract the time from the timestamp\n",
    "df_filtered['time'] = df_filtered['timestamp'].dt.time\n",
    "df_sp['time'] = df_sp['timestamp'].dt.time\n",
    "\n",
    "# Define the start and end times\n",
    "start_time = pd.to_datetime('09:30:00').time()\n",
    "end_time = pd.to_datetime('16:00:00').time()\n",
    "\n",
    "# Filter the rows where the time is between 9:30 and 16:00\n",
    "df_filtered = df_filtered[(df_filtered['time'] >= start_time) & (df_filtered['time'] <= end_time)]\n",
    "df_sp = df_sp[(df_sp['time'] >= start_time) & (df_sp['time'] <= end_time)]\n",
    "\n",
    "# Save data to a parquet file\n",
    "df_filtered.to_parquet(\"./data/raw/raw__nasdaq100_2024.parquet\", engine=\"pyarrow\", index=False)\n",
    "df_sp.to_parquet(\"./data/raw/raw__sp500_2024.parquet\", engine=\"pyarrow\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single stock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aolivest\\AppData\\Local\\Temp\\ipykernel_34352\\1870289914.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  atr = df.groupby('symbol').apply(lambda x: AverageTrueRange(x['high'], x['low'], x['close'], window=14).average_true_range()).reset_index(level=0, drop=True)\n"
     ]
    }
   ],
   "source": [
    "from alpaca.data import StockHistoricalDataClient\n",
    "from alpaca.data.requests import StockBarsRequest\n",
    "from alpaca.data.timeframe import TimeFrame\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Keys\n",
    "API_KEY = os.getenv(\"ALPACA_API_KEY\")\n",
    "SECRET_KEY = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "\n",
    "stock_client = StockHistoricalDataClient(API_KEY,  SECRET_KEY)\n",
    "\n",
    "# ------ Get Stock Data ------\n",
    "stock = \"SPY\"\n",
    "request_params = StockBarsRequest(symbol_or_symbols=[stock], timeframe=TimeFrame.Minute, start=datetime(2025, 1, 1), end=datetime(2025, 4, 1))\n",
    "bars = stock_client.get_stock_bars(request_params)\n",
    "\n",
    "# Flatten the dictionary into a DataFrame\n",
    "df = pd.DataFrame(bars[stock])\n",
    "\n",
    "# Extrahera endast värdena från tuple-paren\n",
    "df = df.map(lambda x: x[1])  # Tar bara det andra elementet i tuple:n\n",
    "\n",
    "# Sätt kolumnnamn\n",
    "df.columns = [\"symbol\", \"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trade_count\", \"vwap\"]\n",
    "\n",
    "# ------ Get SP500 Data ------\n",
    "stock = \"SPY\"\n",
    "request_params = StockBarsRequest(symbol_or_symbols=[stock], timeframe=TimeFrame.Minute, start=datetime(2025, 1, 1), end=datetime(2025, 4, 1))\n",
    "bars = stock_client.get_stock_bars(request_params)\n",
    "\n",
    "# Flatten the dictionary into a DataFrame\n",
    "df_sp = pd.DataFrame(bars[stock])\n",
    "\n",
    "# Extrahera endast värdena från tuple-paren\n",
    "df_sp = df_sp.map(lambda x: x[1])  # Tar bara det andra elementet i tuple:n\n",
    "\n",
    "# Sätt kolumnnamn\n",
    "df_sp.columns = [\"symbol\", \"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trade_count\", \"vwap\"]\n",
    "\n",
    "# ------ Feature Engineering ----\n",
    "df = feature_engineering(df, df_sp)\n",
    "\n",
    "# Save data to a parquet file\n",
    "df.to_parquet(\"./data/test/test__spy.parquet\", engine=\"pyarrow\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several stocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing stock 1/100 - AZN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aolivest\\AppData\\Local\\Temp\\ipykernel_34352\\3014329166.py:41: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  stock_df = pd.concat([stock_df, df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing stock 2/100 - AAPL\n",
      "Processing stock 3/100 - CSCO\n",
      "Processing stock 4/100 - INTC\n",
      "Processing stock 5/100 - MSFT\n",
      "Processing stock 6/100 - WBA\n",
      "Processing stock 7/100 - ATVI\n",
      "Processing stock 8/100 - ADBE\n",
      "Processing stock 9/100 - GOOGL\n",
      "Processing stock 10/100 - AMZN\n",
      "Processing stock 11/100 - AMGN\n",
      "Processing stock 12/100 - ADI\n",
      "Processing stock 13/100 - AMAT\n",
      "Processing stock 14/100 - ADSK\n",
      "Processing stock 15/100 - ADP\n",
      "Processing stock 16/100 - BIDU\n",
      "Processing stock 17/100 - BIIB\n",
      "Processing stock 18/100 - CHTR\n",
      "Processing stock 19/100 - CTSH\n",
      "Processing stock 20/100 - CMCSA\n",
      "Processing stock 21/100 - COST\n",
      "Processing stock 22/100 - CSX\n",
      "Processing stock 23/100 - DLTR\n",
      "Processing stock 24/100 - EBAY\n",
      "Processing stock 25/100 - EA\n",
      "Processing stock 26/100 - FAST\n",
      "Processing stock 27/100 - FISV\n",
      "Processing stock 28/100 - GILD\n",
      "Processing stock 29/100 - ILMN\n",
      "Processing stock 30/100 - INTU\n",
      "Processing stock 31/100 - ISRG\n",
      "Processing stock 32/100 - JD\n",
      "Processing stock 33/100 - KHC\n",
      "Processing stock 34/100 - LRCX\n",
      "Processing stock 35/100 - MAR\n",
      "Processing stock 36/100 - MCHP\n",
      "Processing stock 37/100 - MU\n",
      "Processing stock 38/100 - MDLZ\n",
      "Processing stock 39/100 - MNST\n",
      "Processing stock 40/100 - NTES\n",
      "Processing stock 41/100 - NFLX\n",
      "Processing stock 42/100 - NVDA\n",
      "Processing stock 43/100 - NXPI\n",
      "Processing stock 44/100 - ORLY\n",
      "Processing stock 45/100 - PCAR\n",
      "Processing stock 46/100 - PAYX\n",
      "Processing stock 47/100 - PYPL\n",
      "Processing stock 48/100 - QCOM\n",
      "Processing stock 49/100 - REGN\n",
      "Processing stock 50/100 - ROST\n",
      "Processing stock 51/100 - SIRI\n",
      "Processing stock 52/100 - SWKS\n",
      "Processing stock 53/100 - SBUX\n",
      "Processing stock 54/100 - TMUS\n",
      "Processing stock 55/100 - TSLA\n",
      "Processing stock 56/100 - TXN\n",
      "Processing stock 57/100 - VRSK\n",
      "Processing stock 58/100 - VRTX\n",
      "Processing stock 59/100 - BKNG\n",
      "Processing stock 60/100 - EXC\n",
      "Processing stock 61/100 - HON\n",
      "Processing stock 62/100 - AMD\n",
      "Processing stock 63/100 - ALGN\n",
      "Processing stock 64/100 - AEP\n",
      "Processing stock 65/100 - ANSS\n",
      "Processing stock 66/100 - AVGO\n",
      "Processing stock 67/100 - CDNS\n",
      "Processing stock 68/100 - CTAS\n",
      "Processing stock 69/100 - STZ\n",
      "Processing stock 70/100 - CPRT\n",
      "Processing stock 71/100 - FTNT\n",
      "Processing stock 72/100 - IDXX\n",
      "Processing stock 73/100 - KLAC\n",
      "Processing stock 74/100 - SNPS\n",
      "Processing stock 75/100 - VRSN\n",
      "Processing stock 76/100 - ASML\n",
      "Processing stock 77/100 - DXCM\n",
      "Processing stock 78/100 - ODFL\n",
      "Processing stock 79/100 - PEP\n",
      "Processing stock 80/100 - XEL\n",
      "Processing stock 81/100 - TEAM\n",
      "Processing stock 82/100 - DOCU\n",
      "Processing stock 83/100 - KDP\n",
      "Processing stock 84/100 - LULU\n",
      "Processing stock 85/100 - MRVL\n",
      "Processing stock 86/100 - MTCH\n",
      "Processing stock 87/100 - MELI\n",
      "Processing stock 88/100 - MRNA\n",
      "Processing stock 89/100 - OKTA\n",
      "Processing stock 90/100 - PDD\n",
      "Processing stock 91/100 - SGEN\n",
      "Processing stock 92/100 - SPLK\n",
      "Processing stock 93/100 - WDAY\n",
      "Processing stock 94/100 - ZM\n",
      "Processing stock 95/100 - CRWD\n",
      "Processing stock 96/100 - ABNB\n",
      "Processing stock 97/100 - DDOG\n",
      "Processing stock 98/100 - PANW\n",
      "Processing stock 99/100 - ZS\n",
      "Processing stock 100/100 - META\n",
      "Processing stock 101/100 - LCID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aolivest\\AppData\\Local\\Temp\\ipykernel_34352\\1870289914.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  atr = df.groupby('symbol').apply(lambda x: AverageTrueRange(x['high'], x['low'], x['close'], window=14).average_true_range()).reset_index(level=0, drop=True)\n"
     ]
    }
   ],
   "source": [
    "from alpaca.data import StockHistoricalDataClient\n",
    "from alpaca.data.requests import StockBarsRequest\n",
    "from alpaca.data.timeframe import TimeFrame\n",
    "from pytickersymbols import PyTickerSymbols\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Keys\n",
    "API_KEY = os.getenv(\"ALPACA_API_KEY\")\n",
    "SECRET_KEY = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "\n",
    "# Create client\n",
    "stock_client = StockHistoricalDataClient(API_KEY, SECRET_KEY)\n",
    "\n",
    "# Get all stock symbols in NASDAQ 100\n",
    "stock_data = PyTickerSymbols()\n",
    "nasdaq_100 = stock_data.get_stocks_by_index('NASDAQ 100')\n",
    "\n",
    "# Create empty DataFrame\n",
    "columns = [\"symbol\", \"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trade_count\", \"vwap\"]\n",
    "stock_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "stock_nr = 0\n",
    "for stock in nasdaq_100:\n",
    "    symbol = stock[\"symbol\"]\n",
    "    stock_nr += 1\n",
    "    print(f\"Processing stock {stock_nr}/100 - {symbol}\")\n",
    "\n",
    "    # Get the stock bars\n",
    "    request_params = StockBarsRequest(symbol_or_symbols=[symbol], timeframe=TimeFrame.Minute, start=datetime(2025, 1, 1), end=datetime(2025, 4, 1))\n",
    "    bars = stock_client.get_stock_bars(request_params)\n",
    "    \n",
    "    # Check if the symbol is in the data\n",
    "    if (symbol in bars.data):\n",
    "        # Clean stock data and add to DataFrame\n",
    "        df = pd.DataFrame(bars[stock[\"symbol\"]])\n",
    "        df = df.map(lambda x: x[1]) \n",
    "        df.columns = [\"symbol\", \"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trade_count\", \"vwap\"]\n",
    "        if not df.empty:\n",
    "            stock_df = pd.concat([stock_df, df], ignore_index=True)\n",
    "\n",
    "# ------ Get SP500 Data ------\n",
    "stock = \"SPY\"\n",
    "request_params = StockBarsRequest(symbol_or_symbols=[stock], timeframe=TimeFrame.Minute, start=datetime(2025, 1, 1), end=datetime(2025, 4, 1))\n",
    "bars = stock_client.get_stock_bars(request_params)\n",
    "\n",
    "# Flatten the dictionary into a DataFrame\n",
    "df_sp = pd.DataFrame(bars[stock])\n",
    "\n",
    "# Extrahera endast värdena från tuple-paren\n",
    "df_sp = df_sp.map(lambda x: x[1])  # Tar bara det andra elementet i tuple:n\n",
    "\n",
    "# Sätt kolumnnamn\n",
    "df_sp.columns = [\"symbol\", \"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trade_count\", \"vwap\"]\n",
    "\n",
    "# ------ Feature Engineering ----\n",
    "df = feature_engineering(stock_df, df_sp)\n",
    "\n",
    "df = df.sort_values(by=\"timestamp\", ascending=True)\n",
    "\n",
    "# Save data to a parquet file\n",
    "df.to_parquet(\"./data/test/test__nasdaq_100.parquet\", engine=\"pyarrow\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Stock Symbols In Dataset to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytickersymbols import PyTickerSymbols\n",
    "\n",
    "# Read the parquet file\n",
    "# Get all stock symbols in NASDAQ 100\n",
    "stock_data = PyTickerSymbols()\n",
    "nasdaq_100 = stock_data.get_stocks_by_index('NASDAQ 100')\n",
    "\n",
    "stocks = []\n",
    "for stock in nasdaq_100:\n",
    "    stocks.append(stock[\"symbol\"])\n",
    "\n",
    "# Convert the numpy array to a DataFrame and save as a Parquet file\n",
    "nasdaq_stocks_df = pd.DataFrame(stocks, columns=[\"symbol\"])\n",
    "nasdaq_stocks_df.to_parquet(\"../data/enriched/enriched__nasdaq100_stocks.parquet\", engine=\"pyarrow\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
